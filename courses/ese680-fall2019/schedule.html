<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>ESE 680, Fall 2019 &ndash; Schedule and Course Materials </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">ESE 680, Fall 2019</div>
<div class="menu-item"><a href="index.html">home</a></div>
<div class="menu-item"><a href="schedule.html" class="current">schedule</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>ESE 680, Fall 2019 &ndash; Schedule and Course Materials<br /></h1>
</div>
<h2>Course Materials</h2>
<p>There is no textbook for the course.  Our discussions will be guided by papers, monographs, and lecture notes that are available online.  The following incomplete list will grow:</p>
<ul>
<li><p>[Recht19] <a href="https://www.annualreviews.org/doi/pdf/10.1146/annurev-control-053018-023825">A Tour of Reinforcement Learning: The View from Continuous Control</a>, Recht 2019</p>
</li>
<li><p>[Viberg95] <a href="https://www.sciencedirect.com/science/article/pii/0005109895001075">Subspace-based Methods for the Identification of Linear Time-invariant Systems</a>, Viberg 1995</p>
</li>
<li><p>[MatniAndTu19] <a href="https://arxiv.org/abs/1906.11395">A Tutorial on Concentration Bounds for System Identification</a>, Matni and Tu, 2019</p>
</li>
<li><p>[Rigollet] <a href="https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/MIT18_S997S15_CourseNotes.pdf">Lecture Notes on High-Dimensional Statistics</a>, Rigollet</p>
</li>
<li><p>[DeanEtAl17] <a href="https://arxiv.org/abs/1710.01688">On the Sample Complexity of the Linear Quadratic Regulator</a>, Dean, Mania, Matni, Recht, and Tu, 2017</p>
</li>
<li><p>[Wasserman] <a href="http://www.stat.cmu.edu/~larry/=stat705/Lecture13.pdf">CMU Stats 705, Lecture 13: The Boostrap</a>, Wasserman</p>
</li>
<li><p>[SarkarAndRakhlin19] <a href="http://proceedings.mlr.press/v97/sarkar19a/sarkar19a-supp.pdf">Near optimal finite time identification of arbitrary linear dynamical systems</a>, Sarkar and Rakhlin, 2019</p>
</li>
<li><p>[SimchowitzEtAl18] <a href="https://arxiv.org/abs/1802.08334">Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification</a>, Simchowitz, Mania, Tu, Jordan, and Recht, 2018</p>
</li>
<li><p>[OymakAndOzay19] <a href="https://arxiv.org/abs/1806.05722">Non-asymptotic Identification of LTI Systems from a Single Trajectory</a>, Oymak and Ozay, 2019</p>
</li>
<li><p>[TsiamisAndPappas19] <a href="https://arxiv.org/abs/1903.09122">Finite Sample Analysis of Stochastic System Identification</a>, Tsiamis and Pappas, 2019</p>
</li>
<li><p>[DFT] <a href="https://www.control.utoronto.ca/people/profs/francis/dft.pdf">Feedback Control Theory</a>, Doyle, Francis, and Tannenbaum</p>
</li>
<li><p>[Lall] <a href="http://floatium.stanford.edu/engr210a/lectures/lecture17_2001_12_03_02.pdf">Stanford Engr210a, Lecture 17: LFTs and robustness</a>, Lall</p>
</li>
<li><p>[LessardRechtPackard16] <a href="https://epubs.siam.org/doi/pdf/10.1137/15M1009597">Analysis and design of optimization algorithms via integral quadratic constraints</a>, Lessard, Recth, and Packard, 2016</p>
</li>
<li><p>[Jonsson] <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=4AF5981026ACEC33748BA9D7CD2DDAE9?doi=10.1.1.22.3273&amp;rep=rep1&amp;type=pdf">Lecture Notes on Integral Quadratic Constraints</a>, Jonsson</p>
</li>
<li><p>[LeongDoyle16] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7798480">Understanding Robust Control Theory Via Stick Balancing</a>, Leong and Doyle, 2016 </p>
</li>
<li><p>[LeongDoyle17] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937900">Effects of Delays, Poles, and Zeros on Time Domain Waterbed Tradeoffs and Oscillations</a>, Leong and Doyle, 2017</p>
</li>
<li><p>[SmithDoyle92] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=148346">Model Validation: A Connection between Robust Control and System Identification</a>, Smith and Doyle, 1992</p>
</li>
<li><p>[PoollaEtAl94] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=284871">A Time-Domain Approach to Model Validation</a>, Poolla, Khargonekar, Tikku, Krause, and Nagpal, 1994 </p>
</li>
<li><p>[Prajna05]  <a href="https://www.sciencedirect.com/science/article/pii/S0005109805002839">Barrier Certificates for nonlinear model validation</a></p>
</li>
<li><p>[AndersonEtAl19] <a href="https://arxiv.org/pdf/1904.01634.pdf">System Level Synthesis</a>, Anderson, Doyle, Low, and Matni, 2019</p>
</li>
<li><p>[MatniEtAl19] <a href="https://arxiv.org/abs/1906.11392">From self-tuning regulators to reinforcement learning and back again</a>, Matni, Proutiere, Rantzer, and Tu, 2019</p>
</li>
<li><p>[DannLattimoreBrunskill17] <a href="https://arxiv.org/abs/1703.07710">Unifying PAC and Regret: Uniform PAC Bounds for
Episodic Reinforcement Learning</a>, Dann, Lattimore, and Brunskill, 2017</p>
</li>
<li><p>[AbbasiYadkoriSzepesvari11] <a href="http://proceedings.mlr.press/v19/abbasi-yadkori11a/abbasi-yadkori11a.pdf">Regret Bounds for the Adaptive Control of Linear Quadratic Systems</a>, Abbasi-Yadkori and Szepesari, 2011</p>
</li>
<li><p>[AbeilleLazaric18] <a href="http://proceedings.mlr.press/v80/abeille18a/abeille18a-supp.pdf">Improved Regret Bounds for Thompson Sampling in Linear Quadratic Control Problems</a>, Abeille and Lazaric, 2018</p>
</li>
<li><p>[BousquetElisseeff02] <a href="http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf">Stability and Generalization</a></p>
</li>
<li><p>[HardtRechtSinger16] <a href="https://arxiv.org/abs/1509.01240">Train faster, generalize better: Stability of stochastic gradient descent</a>, Hardt, Recht, and Singer, 2016</p>
</li>
<li><p>[BartlettMendelson02] <a href="http://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf">Rademacher and Gaussian Complexities: Risk Bounds and Structural Results</a></p>
</li>
<li><p>[SrebroSridharanTewari10] <a href="https://www.cs.cornell.edu/~sridharan/smoothness.pdf">Smoothness, Low Noise, and Fast Rates</a>, Srebro, Sridharan, and Tewari, 2010</p>
</li>
<li><p>[SuttonBarto] <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">Reinforcement Learning</a> Sutton and Barto, 2017</p>
</li>
<li><p>[BradtkeYdstieBarto94] <a href="https://ieeexplore.ieee.org/abstract/document/735224">Adaptive linear quadratic control using policy iteration</a>, Bradtke, Ydstie, and Barto, 1994</p>
</li>
<li><p>[DuEtAl2019] <a href="https://arxiv.org/pdf/1910.03016.pdf">Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?</a>, Du, Kakade, Wang, and Yang, 2019</p>
</li>
<li><p>[FazelGeKakadeMesbahi2019] <a href="https://arxiv.org/pdf/1801.05039.pdf">Global Convergence of Policy Gradient
Methods for the Linear Quadratic Regulator</a> Fazel, Ge, Kakade, and Mesbahi, 2019</p>
</li>
<li><p>[TuRecht2018] <a href="https://arxiv.org/abs/1812.03565">The Gap Between Model-Based and Model-Free Methods on the Linear Quadratic Regulator: An Asymptotic Viewpoint</a>, Tu and Recht, 2018</p>
</li>
<li><p>[AmesEtAl19] <a href="https://arxiv.org/abs/1903.11199">Control Barrier Functions: Theory and Applications</a>, Ames, Coogan, Egerstedt, Notomista, Sreenath, and Tabuada, 2019</p>
</li>
<li><p>[BerkenkampEtAl17] <a href="https://arxiv.org/abs/1705.08551">Safe Model-based Reinforcement Learning with Stability Guarantees</a>,  Berkenkamp, Turchetta, Schoellig, and Krause, 2017</p>
</li>
<li><p>[FazlyabRobeyMorariPappas19] <a href="https://arxiv.org/abs/1906.04893">Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks</a> Fazlyab, Robey, Hassani, Morari, and Pappas, 2019</p>
</li>
<li><p>[FazlyabMorariPappas19a] <a href="https://arxiv.org/abs/1910.04249">Probabilistic Verification and Reachability Analysis of Neural Networks via Semidefinite Programming</a>, Fazlyab, Morari, and Pappas, 2019</p>
</li>
<li><p>[FazlyabMorariPappas19b] <a href="https://arxiv.org/abs/1903.01287">Safety Verification and Robustness Analysis of Neural Networks via Quadratic Constraints and Semidefinite Programming</a> Fazlyab, Morari, and Pappas, 2019</p>
</li>
</ul>
<h2>Additional Resources</h2>
<ul>
<li><p>[Ljung] System Identification: Theory for the User, Ljung  (<a href="http://www.control.isy.liu.se/research/reports/2007/2809.pdf">survey paper</a>)</p>
</li>
<li><p>[Vershynin] <a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf">High-Dimensional Probability</a>, Verhsynin</p>
</li>
<li><p>Sanjay Lall's <a href="http://floatium.stanford.edu/engr210a/">Engr201a</a> Robust Control course at Stanford</p>
</li>
<li><p>[ZhouDoyleGlover] Robust and Optimal Control, Zhou, Doyle, and Glover</p>
</li>
<li><p>[ZhouDoyle] Essentials of Robust Control, Zhou and Doyle</p>
</li>
<li><p>[DullerudPaganini] A course in robust control: a convex approach, Dullerud and Paganini</p>
</li>
<li><p>[RussoEtAl17] <a href="https://arxiv.org/abs/1707.02038">A Tutorial on Thompson Sampling</a>, Russo et al, 2017</p>
</li>
<li><p>[Ioannou] <a href="http://www-bcf.usc.edu/~ioannou/RobustAdaptiveBook95pdf/Robust_Adaptive_Control.pdf">Robust Adaptive Control</a>, Ioannou, 1995</p>
</li>
<li><p>[ShalevSchwartzAndBenDavid] <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: from Theory to Algorithms</a></p>
</li>
<li><p>[BousquetEtAl] <a href="http://www.econ.upf.edu/~lugosi/mlss_slt.pdf">Introduction to Statistical Learning Theory</a></p>
</li>
<li><p>[CuckerSmale01] <a href="https://www.ams.org/journals/bull/2002-39-01/S0273-0979-01-00923-5/S0273-0979-01-00923-5.pdf">On the Mathematical Foundations of Learning</a></p>
</li>
<li><p>MIT's <a href="http://www.mit.edu/~9.520/fall18/index.html">9.520 Statistical Learning Theory and Applications</a></p>
</li>
<li><p>[Bertsekas] <a href="https://web.mit.edu/dimitrib/www/RLbook.html">Reinforcement Learning and Optimal Control</a></p>
</li>
<li><p>[BersekasTsitsiklis] <a href="http://athenasc.com/ndpbook.html">Neuro-dynamic Programming</a></p>
</li>
<li><p>[KrauthTuRecht2019] <a href="https://arxiv.org/abs/1905.12842">Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator</a>, Krauth, Tu, and Recth, 2019</p>
</li>
<li><p>[GaussianProcesses] <a href="http://www.gaussianprocess.org/gpml/chapters/">Gaussian Processes for Machine Learning</a>, Rasmussen and Williams, 2006.</p>
</li>
</ul>
<h2>Schedule (subject to change)</h2>
<h3>Logistics</h3>
<ul>
<li><p>Lecture 1: Aug 27</p>
<ul>
<li><p>Welcome, logistics, and overview (<a href="https://www.dropbox.com/s/dreu9wjgk0shutk/Lecture00-Welcome.pptx?dl=0">Slides</a>)</p>
<ul>
<li><p>Reading: <a href="https://www.annualreviews.org/doi/pdf/10.1146/annurev-control-053018-023825">Sections 1-3 of Recht19</a></p>
</li>
<li><p><a href="https://www.surveymonkey.com/r/XFSR26N">Background Poll</a></p>
</li>
<li><p><a href="https://docs.google.com/spreadsheets/d/1Kqa-ZnnqO-r7-c0rTKNSaCNiz_hTQARH9V_Ug8blp3Q/edit?usp=sharing">Sign up sheet</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<h3>System Identification</h3>
<ul>
<li><p>Lecture 2: Aug 29</p>
<ul>
<li><p>System identification 1: Identification of Linear-Time-Invariant systems</p>
<ul>
<li><p>Reading: <a href="https://www.sciencedirect.com/science/article/pii/0005109895001075">Viberg95</a></p>
</li>
<li><p>Methods/algorithms: Subspace methods, Ho-Kalman</p>
</li>
<li><p>Scribe: Duc Nguyen, <a href="scribe-notes/lecture02.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 3:  Sep 03 </p>
<ul>
<li><p>System identification 2: concentration bounds and finite-data guarantees, full state iid case</p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/1906.11395">Sections 1-3 of MatniAndTu19</a>, <a href="https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/">Ch1 of Rigollet Lecture Notes</a></p>
</li>
<li><p>Methods/algorithms: Ordinary Least Squares error analysis</p>
</li>
<li><p>Scribe: Zongyu Dai, <a href="scribe-notes/lecture03.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 4: Sep 05</p>
<ul>
<li><p>System identification 3: finite-data guarantees (cont'd) &amp; data-dependent bounds</p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/1906.11395">Section 5 of MatniAndTu19</a>, <a href="https://arxiv.org/abs/1710.01688">Prop 2.4 and Section 2.3 of DeanEtAl17</a>, <a href="http://www.stat.cmu.edu/~larry/=stat705/Lecture13.pdf">Wasserman</a></p>
</li>
<li><p>Methods/Algorithms: Ordinary Least Squares error analysis, The Bootstrap</p>
</li>
<li><p>Scribe: Kuk Jang, <a href="scribe-notes/lecture04.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 5: Sep 10</p>
<ul>
<li><p>System identification 4: student led discussion, full state single trajectory case</p>
<ul>
<li><p>Paper: Choose one of <a href="http://proceedings.mlr.press/v97/sarkar19a/sarkar19a-supp.pdf">SarkarAndRakhlin19</a>, <a href="https://arxiv.org/abs/1802.08334">SimchowitzEtAl18</a></p>
</li>
<li><p>Presenter:  Bernadette K. Bucher</p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 6: Sep 12</p>
<ul>
<li><p>System identification 5: student led discussion, partial state single trajectory case</p>
<ul>
<li><p>Paper: Choose one of <a href="https://arxiv.org/abs/1806.05722">OymakAndOzay19</a>, <a href="https://arxiv.org/abs/1903.09122">TsiamisAndPappas19</a></p>
</li>
<li><p>Presenter: Alex Robey</p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Additional resources:</p>
<ul>
<li><p>[Ljung] System Identification: Theory for the User, Ljung  (<a href="http://www.control.isy.liu.se/research/reports/2007/2809.pdf">survey paper</a>)</p>
</li>
<li><p>[Vershynin] <a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf">High-Dimensional Probability</a>, Verhsynin</p>
</li>
</ul>

</li>
</ul>
<h3>Control of Uncertain Systems </h3>
<ul>
<li><p>Lecture 7: Sep 17</p>
<ul>
<li><p>Control of Uncertain Systems 1: introduction to optimal/robust control, modeling uncertainty, small gain theorem</p>
<ul>
<li><p>Reading: <a href="http://www.cds.caltech.edu/~macmardg/courses/cds110b/dft/dft92-ch2.pdf">DFT Chapter 2</a>, <a href="http://floatium.stanford.edu/engr210a/lectures/lecture17_2001_12_03_02.pdf">Lecture 17 of Lall</a></p>
</li>
<li><p>Methods/Algorithms: LQR, H-inf and L1 optimal control, LFT respresentation of uncertainty, small gain theorem</p>
</li>
<li><p>Scribe: Christopher Hsu, <a href="scribe-notes/lecture07.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 8: Sep 19</p>
<ul>
<li><p>Control of Uncertain Systems 2: small gain theorem, a <b>very</b> brief introduction to the structured singular value (mu) and integral quadratic constraints (IQCs)</p>
<ul>
<li><p>Reading: Chapter 8 of [DullerudPaganini], <a href="https://epubs.siam.org/doi/pdf/10.1137/15M1009597">Sections 3.1-3.3 of LessardRechtPackard16</a>, (optional reading: <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=4AF5981026ACEC33748BA9D7CD2DDAE9?doi=10.1.1.22.3273&amp;rep=rep1&amp;type=pdf">Jonsson IQC Lecture Notes</a>)</p>
</li>
<li><p>Methods/Algorithms: small gain theorem, structured singular value, KYP Lemma, IQCs</p>
</li>
<li><p>Scribe: Kendall Queen, <a href="scribe-notes/lecture08.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 9: Sep 24</p>
<ul>
<li><p>Control of Uncertain Systems 3: student led presentation on fundamental limits of robust control</p>
<ul>
<li><p>Paper: Both <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7798480">LeongDoyle16</a> and <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937900">LeongDoyle17</a></p>
</li>
<li><p>Presenter: Anton Xue</p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 10: Sep 26</p>
<ul>
<li><p>Control of Uncertain Systems 4: student led presentation on model (in)validation from a robust control perspective</p>
<ul>
<li><p>Paper: either <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=148346">SmithDoyle92</a> or <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=284871">PoollaEtAl94</a> <b>and</b> <a href="https://www.sciencedirect.com/science/article/pii/S0005109805002839">Prajna05</a></p>
</li>
<li><p>Presenter: Laura Jarin-Lipschitz</p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Additional Resources:</p>
<ul>
<li><p>Sanjay Lall's <a href="http://floatium.stanford.edu/engr210a/">Engr201a</a> course at Stanford</p>
</li>
<li><p>[ZhouDoyleGlover] Robust and Optimal Control, Zhou, Doyle, and Glover</p>
</li>
<li><p>[ZhouDoyle] Essentials of Robust Control, Zhou and Doyle</p>
</li>
</ul>

</li>
</ul>
<h3>Model-based control of learned systems</h3>
<ul>
<li><p>Lecture 11: Oct 01</p>
<ul>
<li><p>Model-based control of learned systems 1: interpretable robust control with System Level Synthesis and end-to-end bounds for learning to control an unknown linear dynamical system</p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/pdf/1904.01634.pdf">Sections 2, 3, 4.0 and 4.5 of AndersonEtAl19</a>, <a href="https://arxiv.org/abs/1710.01688">Sections 3 and 4 of DeanEtAl17</a></p>
</li>
<li><p>Methods/Algorithms: system level synthesis, quantitative performance bounds, end-to-end bounds</p>
</li>
<li><p>Scribe: Haimin Hu, <a href="scribe-notes/lecture11.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 12: 0ct 03</p>
<ul>
<li><p>Model-based control of learned systems 2: PAC, regret, and beyond, and what do we know about learning to control the linear quadratic regulator</p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/1906.11392">Section 3 of MatniEtAl19</a>, <a href="https://arxiv.org/abs/1703.07710">Sections 1 and 2 of DannLattimoreBrunskill17</a></p>
</li>
<li><p>Methods/Algorithms: episodic and single-trajecotry PAC and Regret bounds</p>
</li>
<li><p>Scribe: Klayton Wittler, <a href="scribe-notes/lecture12.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 13: Oct 08</p>
<ul>
<li><p>Model-based control of learned systems 3: student led presentation on Optimism in the Face of Uncertainty (OFU) for LQR</p>
</li>
<li><p>Paper: <a href="http://proceedings.mlr.press/v19/abbasi-yadkori11a/abbasi-yadkori11a.pdf">AbbasiYadkoriSzepesvari11</a></p>
</li>
<li><p>Presenter: Shaoru Chen</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 14: Oct 15</p>
<ul>
<li><p>Model-based control of learned systems 4: student led presentation on Thompson Sampling for LQR</p>
</li>
<li><p>Paper: <a href="http://proceedings.mlr.press/v80/abeille18a/abeille18a-supp.pdf">AbeilleLazaric18</a></p>
</li>
<li><p>Presenter: Rebecca Li</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Additional Resources:</p>
<ul>
<li><p>[RussoEtAl17] <a href="https://arxiv.org/abs/1707.02038">A Tutorial on Thompson Sampling</a>, Russo et al, 2017</p>
</li>
<li><p>[Ioannou] <a href="http://www-bcf.usc.edu/~ioannou/RobustAdaptiveBook95pdf/Robust_Adaptive_Control.pdf">Robust Adaptive Control</a>, Ioannou, 1995</p>
</li>
</ul>

</li>
</ul>
<h3>Learning Theory</h3>
<ul>
<li><p>Lecture 15: Oct 17</p>
<ul>
<li><p>Learning theory 1: Empirical Risk Minimization and Uniform Convergence</p>
<ul>
<li><p>Reading: <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Chapters 2-4 ShalevSchwartzAndBenDavid</a>, </p>
</li>
<li><p>Methods/Algorithms: ERM, uniform convergence for bounded loss functions and finite hypothesis classes, and bounded and Lipschitz loss functions and compact hypothesis classes.</p>
</li>
<li><p>Scribe:  Shuo Li, <a href="scribe-notes/lecture15.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 16: Oct 22</p>
<ul>
<li><p>Learning theory 2:  Algorithmic Stability and Stochastic Gradient Descent</p>
<ul>
<li><p>Reading: <a href="http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf">BousquetElisseeff02</a>, <a href="https://arxiv.org/abs/1509.01240">HardtRechtSinger16</a></p>
</li>
<li><p>Methods/Algorithms: Generalization through Algorithmic Stability, Stability and generalization of SGD</p>
</li>
<li><p>Scribe: Jialin Mao, <a href="scribe-notes/lecture16.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 17: Oct 24</p>
<ul>
<li><p>Learning theory 3: student led presentation on Rademacher and Gaussian Complexities for Risk Bounds</p>
<ul>
<li><p>Paper: <a href="http://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf">BartlettMendelson02</a></p>
</li>
<li><p>Presenter: Alexandre Amice</p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 18: Oct 29</p>
<ul>
<li><p>Learning theory 4: student led presentation on Smoothness, Low Noise, and Fast Rates</p>
<ul>
<li><p>Paper: <a href="https://www.cs.cornell.edu/~sridharan/smoothness.pdf">SrebroSridharanTewari10</a></p>
</li>
<li><p>Presenter: Han Wang</p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Additional Resources:</p>
<ul>
<li><p>[BousquetEtAl] <a href="http://www.econ.upf.edu/~lugosi/mlss_slt.pdf">Introduction to Statistical Learning Theory</a></p>
</li>
<li><p>[CuckerSmale01] <a href="https://www.ams.org/journals/bull/2002-39-01/S0273-0979-01-00923-5/S0273-0979-01-00923-5.pdf">On the Mathematical Foundations of Learning</a></p>
</li>
<li><p>MIT's <a href="http://www.mit.edu/~9.520/fall18/index.html">9.520 Statistical Learning Theory and Applications</a></p>
</li>
</ul>

</li>
</ul>
<h3>Model Free Methods</h3>
<ul>
<li><p>Lecture 19: Oct 31</p>
<ul>
<li><p>Model free methods 1</p>
<ul>
<li><p>Reading: <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">SuttonBarto Chapter 6.1-6.5</a>, <a href="https://ieeexplore.ieee.org/abstract/document/735224">BradtkeYdstieBarto94</a>, Optional: <a href="https://arxiv.org/abs/1905.12842">KrauthTuRecht2019</a></p>
</li>
<li><p>Methods/Algorithms: TD-learning, Q-learning, policy iteration</p>
</li>
<li><p>Scribe: Raphael Van Hoffelen, <a href="scribe-notes/lecture19.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 20: Nov 05</p>
<ul>
<li><p>Model free methods 2: student led presentation on if good representations are sufficient for efficient reinforcement living.</p>
<ul>
<li><p>Paper: <a href="https://arxiv.org/pdf/1910.03016.pdf">DuEtAl2019</a></p>
</li>
<li><p>Presenter: Karl Schmeckpeper</p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 21: Nov 07</p>
<ul>
<li><p>Model free methods 3</p>
<ul>
<li><p>Reading: <a href="https://www.annualreviews.org/doi/pdf/10.1146/annurev-control-053018-023825">Sections 3.3, 4.2, 5 of Recht19</a>, <a href="https://arxiv.org/pdf/1801.05039.pdf">FazelGeKakadeMesbahi2019</a>, Optional: a great talk by Maryam Fazel on <a href="https://www.youtube.com/watch?v=5hMhGM0NOHI">gradient based methods for linear control</a> from L4DC 2019.</p>
</li>
<li><p>Methods/Algorithms: REINFORCE, Policy Gradient, Random Search</p>
</li>
<li><p>Scribe: Walker Gosrich, <a href="scribe-notes/lecture21.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 22: Nov 12</p>
<ul>
<li><p>Model free methods 4: student led presentation on the gap between model-free and model-based methods for LQR</p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/1812.03565">TuRecht2018</a></p>
</li>
<li><p>Presenter: Maria-Elisabeth Tzes</p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Additional Resources:</p>
<ul>
<li><p>[Bertsekas] <a href="https://web.mit.edu/dimitrib/www/RLbook.html">Reinforcement Learning and Optimal Control</a></p>
</li>
<li><p>[BersekasTsitsiklis] <a href="http://athenasc.com/ndpbook.html">Neuro-dynamic Programming</a></p>
</li>
<li><p>[KrauthTuRecht2019] <a href="https://arxiv.org/abs/1905.12842">Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator</a>, Krauth, Tu, and Recth, 2019</p>
</li>
</ul>

</li>
</ul>
<h3>Safe Learning and Control</h3>
<ul>
<li><p>Lecture 23: Nov 14</p>
<ul>
<li><p>Safe learning and control 1</p>
<ul>
<li><p>Reading: <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-241j-dynamic-systems-and-control-spring-2011/readings/MIT6_241JS11_chap13.pdf">Basic Lyapunov Theory I</a>, <a href="https://stanford.edu/class/ee363/lectures/lyap.pdf">Basic Lyapunov Theory II</a>, <a href="https://stanford.edu/class/ee363/lectures/io-lyap.pdf">Control Lyapunov Functions</a></p>
</li>
<li><p>Methods/Algorithms: Lyapunov functions, Control Lyapunov functions, Global Asymptotic Stability, Global Exponential Stability</p>
</li>
<li><p>Scribe: Siddharth Singh, </p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 24: Nov 19</p>
<ul>
<li><p>Safe learning and control 2</p>
<ul>
<li><p>Reading: <a href="https://stanford.edu/class/ee363/lectures/estim.pdf">Estimation</a>, <a href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf">Regression using Gaussian Processes</a></p>
</li>
<li><p>Methods/Algorithms: Gaussian Processes</p>
</li>
<li><p>Scribe: Alena Rodionova, <a href="scribe-notes/lecture24.pdf">lecture notes</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 25: Nov 21</p>
<ul>
<li><p>Safe learning and control 3: student led presentation on control barrier functions</p>
<ul>
<li><p>Paper: <a href="https://arxiv.org/abs/1903.11199">AmesEtAl19</a></p>
</li>
<li><p>Presenter: Hyunjoo Oh </p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 26: Nov 26</p>
<ul>
<li><p>Safe learning and control 4: student led presentation on safe reinforcement learning with stability guarantees</p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/1705.08551">BerkenkampEtAl17</a></p>
</li>
<li><p>Presenter: Mengyuan Li</p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 26*: Nov 27</p>
<ul>
<li><p>Bonus lecture on safety and verification of Deep Neural Nets</p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/1906.04893">FazlyabRobeyMorariPappas19</a> <a href="https://arxiv.org/abs/1910.04249">FazlyabMorariPappas19a</a>, <a href="https://arxiv.org/abs/1903.01287">FazlyabMorariPappas19b</a></p>
</li>
<li><p>Presenter: <a href="https://www.seas.upenn.edu/~mahyarfa/">Mahyar Fazlyab</a></p>
</li>
</ul>

</li>
</ul>

</li>
</ul>
<h3>Final Project Presentations</h3>
<ul>
<li><p>Lecture 27: Dec 03</p>
<ul>
<li><p>Final project presentations 1 </p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Lecture 28: Dec 05</p>
<ul>
<li><p>Final project presentations 2</p>
</li>
</ul>

</li>
</ul>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-49484039-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-49484039-2');
</script>
<div id="footer">
<div id="footer-text">
Page generated 2020-01-15 18:32:55 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
