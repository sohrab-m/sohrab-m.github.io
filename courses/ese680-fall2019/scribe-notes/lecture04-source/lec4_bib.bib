%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for jangkj at 2019-11-01 17:30:39 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@article{dean_sample_2017,
	Abstract = {This paper addresses the optimal control problem known as the Linear Quadratic Regulator in the case when the dynamics are unknown. We propose a multi-stage procedure, called Coarse-ID control, that estimates a model from a few experimental trials, estimates the error in that model with respect to the truth, and then designs a controller using both the model and uncertainty estimate. Our technique uses contemporary tools from random matrix theory to bound the error in the estimation procedure. We also employ a recently developed approach to control synthesis called System Level Synthesis that enables robust control design by solving a quasiconvex optimization problem. We provide end-to-end bounds on the relative error in control cost that are optimal in the number of parameters and that highlight salient properties of the system to be controlled such as closed-loop sensitivity and optimal control magnitude. We show experimentally that the Coarse-ID approach enables efficient computation of a stabilizing controller in regimes where simple control schemes that do not take the model uncertainty into account fail to stabilize the true system.},
	Author = {Dean, Sarah and Mania, Horia and Matni, Nikolai and Recht, Benjamin and Tu, Stephen},
	File = {Dean et al_2017_On the Sample Complexity of the Linear Quadratic Regulator.pdf:/Users/jangkj/Google Drive/zotero/___Courses/__Fall 2019/ESE680 Fall 2019 - Matni Course/Scribe References/Dean et al_2017_On the Sample Complexity of the Linear Quadratic Regulator.pdf:application/pdf},
	Journal = {arXiv:1710.01688 [cs, math, stat]},
	Keywords = {Statistics - Machine Learning, Mathematics - Optimization and Control, Computer Science - Machine Learning},
	Language = {en},
	Month = oct,
	Note = {arXiv: 1710.01688},
	Title = {On the {Sample} {Complexity} of the {Linear} {Quadratic} {Regulator}},
	Url = {http://arxiv.org/abs/1710.01688},
	Urldate = {2019-09-10},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1710.01688}}

@article{matni_tutorial_2019,
	Abstract = {We provide a brief tutorial on the use of concentration inequalities as they apply to system identification of state-space parameters of linear time invariant systems, with a focus on the fully observed setting. We draw upon tools from the theories of large-deviations and self-normalized martingales, and provide both data-dependent and independent bounds on the learning rate.},
	Author = {Matni, Nikolai and Tu, Stephen},
	File = {arXiv.org Snapshot:/Users/jangkj/Zotero/storage/ELRSA5ZM/1906.html:text/html;Matni_Tu_2019_A Tutorial on Concentration Bounds for System Identification.pdf:/Users/jangkj/Google Drive/zotero/___Courses/__Fall 2019/ESE680 Fall 2019 - Matni Course/Scribe References/Matni_Tu_2019_A Tutorial on Concentration Bounds for System Identification.pdf:application/pdf},
	Journal = {arXiv:1906.11395 [cs, math, stat]},
	Keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	Month = aug,
	Note = {arXiv: 1906.11395},
	Title = {A {Tutorial} on {Concentration} {Bounds} for {System} {Identification}},
	Url = {http://arxiv.org/abs/1906.11395},
	Urldate = {2019-11-01},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1906.11395}}

@article{efron_bootstrap_1979,
	Abstract = {We discuss the following problem: given a random sample X=(X1,X2,⋯,Xn)X=(X1,X2,⋯,Xn){\textbackslash}mathbf\{X\} = (X\_1, X\_2, {\textbackslash}cdots, X\_n) from an unknown probability distribution FFF, estimate the sampling distribution of some prespecified random variable R(X,F)R(X,F)R({\textbackslash}mathbf\{X\}, F), on the basis of the observed data xx{\textbackslash}mathbf\{x\}. (Standard jackknife theory gives an approximate mean and variance in the case R(X,F)=θ(F̂ )−θ(F),θR(X,F)=θ(F{\textasciicircum})−θ(F),θR({\textbackslash}mathbf\{X\}, F) = {\textbackslash}theta({\textbackslash}hat\{F\}) - {\textbackslash}theta(F), {\textbackslash}theta some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
	Author = {Efron, B.},
	Doi = {10.1214/aos/1176344552},
	File = {Efron_1979_Bootstrap Methods.pdf:/Users/jangkj/Google Drive/zotero/___Courses/__Fall 2019/ESE680 Fall 2019 - Matni Course/Scribe References/Efron_1979_Bootstrap Methods.pdf:application/pdf;Snapshot:/Users/jangkj/Zotero/storage/MYDU6XGW/1176344552.html:text/html},
	Issn = {0090-5364, 2168-8966},
	Journal = {The Annals of Statistics},
	Keywords = {bootstrap, discriminant analysis, error rate estimation, Jackknife, nonlinear regression, nonparametric variance estimation, resampling, subsample values},
	Language = {EN},
	Month = jan,
	Mrnumber = {MR515681},
	Number = {1},
	Pages = {1--26},
	Shorttitle = {Bootstrap {Methods}},
	Title = {Bootstrap {Methods}: {Another} {Look} at the {Jackknife}},
	Url = {https://projecteuclid.org/euclid.aos/1176344552},
	Urldate = {2019-11-01},
	Volume = {7},
	Year = {1979},
	Zmnumber = {0406.62024},
	Bdsk-Url-1 = {https://projecteuclid.org/euclid.aos/1176344552},
	Bdsk-Url-2 = {https://doi.org/10.1214/aos/1176344552}}

@misc{cmu_lect_13,
	Date-Modified = {2019-11-01 17:30:28 -0400},
	File = {Lecture13.pdf:/Users/jangkj/Zotero/storage/UKSJAJF7/Lecture13.pdf:application/pdf},
	Title = {Lecture13.pdf},
	Url = {http://www.stat.cmu.edu/~larry/=stat705/Lecture13.pdf},
	Urldate = {2019-11-01},
	Bdsk-Url-1 = {http://www.stat.cmu.edu/~larry/=stat705/Lecture13.pdf}}
